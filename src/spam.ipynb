{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_data = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\")\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[\"is_spam\"] = total_data[\"is_spam\"].apply(lambda x: 1 if x else 0).astype(int)\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_data.shape)\n",
    "total_data = total_data.drop_duplicates()\n",
    "total_data = total_data.reset_index(inplace = False, drop = True)\n",
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spam: {len(total_data.loc[total_data.is_spam == 1])}\")\n",
    "print(f\"No spam: {len(total_data.loc[total_data.is_spam == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove any character that is not a letter (a-z) or white space ( )\n",
    "    text = re.sub(r'[^a-z ]', \" \", text)\n",
    "    \n",
    "    # Remove white spaces\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', \" \", text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', \" \", text)\n",
    "\n",
    "    # Multiple white spaces into one\n",
    "    text = re.sub(r'\\s+', \" \", text.lower())\n",
    "\n",
    "    # Remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "\n",
    "    return text.split()\n",
    "\n",
    "total_data[\"url\"] = total_data[\"url\"].apply(preprocess_text)\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def lemmatize_text(words, lemmatizer = lemmatizer):\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    return tokens\n",
    "\n",
    "total_data[\"url\"] = total_data[\"url\"].apply(lemmatize_text)\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, background_color = \"black\", max_words = 1000, min_font_size = 20, random_state = 42)\\\n",
    "    .generate(str(total_data[\"url\"]))\n",
    "\n",
    "fig = plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokens_list = total_data[\"url\"]\n",
    "tokens_list = [\" \".join(tokens) for tokens in tokens_list]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 5000, max_df = 0.8, min_df = 5)\n",
    "X = vectorizer.fit_transform(tokens_list).toarray()\n",
    "y = total_data[\"is_spam\"]\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel = \"linear\", random_state = 42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparams = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputedâ€™\"],\n",
    "    \"degree\": [1, 2, 3, 4, 5],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# We initialize the random search\n",
    "grid = GridSearchCV(model, hyperparams, scoring = \"accuracy\", cv = 5)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best hyperparameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model = SVC(C = 1000, degree = 1, gamma = \"auto\", kernel = \"poly\", random_state = 42)\n",
    "opt_model.fit(X_train, y_train)\n",
    "y_pred = opt_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "dump(model, open(\"/models/svm_classifier_C-1000_deg-1_gam-auto_ker-poly_42.sav\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
